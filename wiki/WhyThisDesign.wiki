#summary One-sentence summary of this page.

*1. Why aren't there any "codes" in the document instances? I want codes!*

Ah, but there are! There are no obscure numeric codes, but all the elements in this design reside in an XML namespace ({{{http://purl.oclc.org/medical/report/}}} or namespaces that are derived from this). This means that the full name (or to use the official XML term, the namespace-qualified name) of (for example) the element that wraps information about the tumor histologic type in this project is:

     {{{http://purl.oclc.org/medical/reporting/cap-cancer/histologicType}}}

Is that sufficiently "code-like" to suit you?

For those unfamiliar with XML, you just have to realize that because namespace-qualified names can be long and human-unfriendly (like any "codes"), the XML standard allows them to be "abbreviated" in schemas by declaring a default namespace in the document. This is what we do.

(One sidelight for XML connoisseurs: notice that while all our element names are in specific namespaces, the associated attributes are in no-namespace (i.e. the "" namespace). We could have put our attributes in a namespace (and it's a discussable point), but at the moment we believe that since these attributes only exist in association with a "carrying' element, namespace restriction is unnecessary for them.)

*2. Why doesn't the document use SNOMED-CT (or LOINC, or UMLS, or RadLex) codes instead of your own XML element names? After all those coding systems are more widely used, whereas your system is novel!*

Unlike question #1, which is purely about technical implementation, this gets to the heart of the design philosophy. 

We recognize (at least) two approaches to making documents machine-interoperable. One is to compose them directly in some existing, standardized, "ontology" that is administered by someone else. The other is to compose them in a public, unambiguous, but use-case specific vocabulary and then (in a separate step) map to one or more shared "ontologies".

The (perceived) advantage of the first approach is that (if you're lucky) you'll find some "ready-made" vocabulary items that you can "bake-in" to your design, thus possibly saving yourself some work downstream. The real-world disadvantages, however, are that:

  # typically you _don't_ find all the vocabulary items you need to cover a specialized use case. You need to go to the "owners" of the vocabulary and request that your items be added. This takes time, and may involve a balloting process. The ontology owners may disagree with your semantics, and give you some terms that are similar to what you requested, but not exactly the same. 
  # the vocabulary items you requested may be highly specific to your use case and of no interest to the general users of big ontologies. When thousands of such requests are made, the central ontology suffers "bloat". The mere accretion of more terms does not add quality to the central ontology unless the new terms are accurately "hooked up" with existing terms. But the central ontology administrators, who are not experts in every use case, may not be able to do this. Or they may simply be overwhelmed by the number of term requests and enter them into the ontology perforce with very little "modeling", i.e. very few connections to existing terms. The central ontology begins to look like a flat term list with hundreds of thousands of terms that no one can possibly curate.
  # central ontologies typically make some claim of "global semantics"; this project is an example of a highly specialized, use-case specific standard (although it's an important use-case!)

  When I say "global semantics" I mean that by virtue of being central and serving every possible use case, a central
  ontology needs to define terms in ways that "reconcile" differences in understanding among different user 
  communities. If such reconciliation is not enforced, then the central ontology will come to contain logically 
  incompatible assertions, and will ultimately become "unclassifiable" (i.e. it will no longer "compute"). But in the real
  world, subtle and not-so-subtle differences in understanding of vocabulary among user communities are a fact of
  life. Ongoing negotiation of such differences is the very essence of genuine human language. Such differences arise
  and recede constantly as knowledge and practice evolve. Large global ontologies always provide, therefore, at best 
  a temporal "snapshot" and a conceptual "average" of how vocabulary is used and how concepts are understood by       
  users. (In the semantic web world, this dynamic character of language has been recognized, and a variety of 
  techniques have been proposed to accommodate it. For example, you might want to google "Named Graphs".) The  
  more specialized the use case, the more violence the reconciliation process may do to the conceptualizations of the 
  specialized users. For example, pathologists have a particular way of looking at tumors, particular understandings 
  of terms like "size", "extent", "differentiation" and so on, that are overlapping, but also subtly or not-so-subtly 
  different from the understanding of, say, radiologists or oncologists or molecular biology researchers. (In fact, a lot 
  of what a surgical pathologist spends her day doing, is talking to other medical specialists and explicating precisely 
  these matters so her meaning is appropriately understood by other specialists.)